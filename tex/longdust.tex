\documentclass[webpdf,contemporary,large,namedate]{oup-authoring-template}%

%\PassOptionsToPackage{hyphens}{url}
%\PassOptionsToPackage{colorlinks,linkcolor=blue,urlcolor=blue,citecolor=blue,anchorcolor=blue}{hyperref}

\DeclareMathOperator*{\argmax}{argmax}

\usepackage{algorithmicx}
\usepackage{lmodern}
\usepackage{setspace}
\renewcommand{\ttdefault}{cmtt}

\begin{document}
\journaltitle{TBD}
\DOI{TBD}
\copyrightyear{2025}
\pubyear{2025}
\access{Advance Access Publication Date: Day Month Year}
\appnotes{Preprint}
\firstpage{1}

\title[Finding low-complexity sequences]{Finding low-complexity DNA sequences with longdust}
\author[1,2,3,$\ast$]{Heng Li\ORCID{0000-0003-4874-2874}}
\author[4]{Brian Li}
\address[1]{Department of Biomedical Informatics, Harvard Medical School, 10 Shattuck St, Boston, MA 02215, USA}
\address[2]{Department of Data Science, Dana-Farber Cancer Institute, 450 Brookline Ave, Boston, MA 02215, USA}
\address[3]{Broad Insitute of MIT and Harvard, 415 Main St, Cambridge, MA 02142, USA}
\address[4]{Commonwealth School, Boston, MA 02116, USA}
\corresp[$\ast$]{Corresponding author. \href{mailto:hli@ds.dfci.harvard.edu}{hli@ds.dfci.harvard.edu}}

%\received{Date}{0}{Year}
%\revised{Date}{0}{Year}
%\accepted{Date}{0}{Year}

\abstract{
\sffamily\footnotesize
\textbf{Motivation:}
Low-complexity (LC) DNA sequences are compositionally repetitive sequences
that are often associated with increased variant density and variant calling artifacts.
While algorithms for identifying LC sequences exist,
they either lack rigorous mathematical foundation
or are inefficient with long context windows.
\vspace{0.5em}\\
\textbf{Results:}
Longdust is a new algorithm that efficiently identifies long LC sequences including centromeric satellite
and tandem repeats with moderately long motifs.
It defines string complexity by statistically modeling the $k$-mer count distribution
with the parameters: the $k$-mer length, the context window size and a threshold on complexity.
Longdust exhibits high performance on real data and high consistency with existing methods.
\vspace{0.5em}\\
\textbf{Availability and implementation:}
\url{https://github.com/lh3/longdust}
}

\maketitle

\section{Introduction}

In computer science, a string is of low complexity (LC) if it is repetitive in composition.
LC strings tend to be tandemly repetitive and
most of them can be identified with tandem repeat finding algorithms such as
TRF~\citep{Benson:1999aa}, TANTAN~\citep{Frith:2011aa}, ULTRA~\citep{Olson:2024aa} and pytrf~\citep{Du:2025aa}.
These algorithms do not rigorously define string complexity.
They rely on heuristics to search for impure tandem repeats
and cannot identify LC strings without clear tandem structures.
SDUST~\citep{Morgulis:2006aa} is the only widely used algorithm that explicitly defines string complexity
and finds the exact solutions.
However, with $O(w^3L)$ time complexity, where $w$ is the window size and $L$ is the genome length,
SDUST is inefficient given a large $w$ and is thus impractical for finding satellite or tandem repeats with long motifs.
Furthermore, the complexity scoring function used by SDUST is not backed by rigorous modeling.
The theoretical properties of LC strings defined this way are unclear.

Inspired by SDUST, we sought an alternative way to define the $k$-mer complexity of a string
and to identify LC regions in a genome.
Our complexity scoring function is based on a statistical model of $k$-mer count distribution
and our algorithm is practically close to $O(wL)$ in time complexity,
enabling the efficient identification of LC strings in long context windows.

\section{Methods}

Similar to SDUST~\citep{Morgulis:2006aa}, we define the complexity of a DNA string with a function of the $k$-mer counts of the string.
In this section, we will first model the $k$-mer count distribution of random strings.
We will then describe the complexity scoring function and the condition on bounding LC substrings in a long string.
We will compare our method to SDUST in the end.

\subsection{Notations}

Let $\Sigma=\{{\tt A},{\tt C},{\tt G},{\tt T}\}$ be the DNA alphabet,
$x\in\Sigma^*$ is a DNA string and $|x|$ is its length.
$t\in\Sigma^k$ is a $k$-mer.
For $|x|\ge k$, $c_x(t)$ is the occurrence of $k$-mer $t$ in $x$;
$\ell(x)=\sum_t c_x(t)=|x|-k+1$ is the total number of $k$-mers in $x$.
$\vec{c}_x$ denotes the count array over all $k$-mers.

In this article, we assume there is one long genome string of length $L$.
We use closed interval $[i,j]$ to denote the substring starting at $i$
and ending at $j$, including the end points $i$ and $j$.
We may use ``interval'' and ``subsequence'' interchangeably.

\subsection{Modeling $k$-mer counts}

Suppose symbols in $\Sigma$ all occur at equal frequency.
Then for all $k$-mer $t$, $c_x(t)\sim{\rm Poisson}(\lambda)$ where $\lambda=\ell(x)/4^k$.
Let
$$
p(n|\lambda)\triangleq\frac{\lambda^n}{n!}e^{-\lambda}
$$
be the probability mass function of Poisson distribution.
Notably, although $c_x(t)\le\ell(x)$, given that $\ell(x)\gg1$ in practice,
$$
p(\ell|\lambda)\approx\frac{e^{-\lambda}}{\sqrt{2\pi\ell}}\cdot\left(\frac{e}{4^k}\right)^{\ell}\ll 1
$$
with the Sterling formula -- $p(\ell|\lambda)$ is very close to 0.
This suggests Poisson remains a good approximation.

The composite probability of string $x$ can be modeled by
$$
P(\vec{c}_x)=\prod_{t\in\Sigma^k}p(c_x(t)|\lambda)
$$
We have
\begin{equation}\label{eq:P}
\log P(\vec{c}_x)=4^k\lambda(\log\lambda-1)-\sum_t\log c_x(t)!
\end{equation}
To get an intuition about $P(\vec{c}_x)$, suppose $\ell(x)\ll4^k$.
In this case, $c_x(t)$ will be mostly 0 or 1 for a random string
and the last term in Eq.~(\ref{eq:P}) will be close to 0.
Given an LC string of the same length,
we will see more $c_x(t)$ of 2 or higher, which will reduce $\log P(\vec{c}_x)$.
Thus the probability of an LC string is lower under this model.

Although $\log P(\vec{c}_x)$ can be used to compare the complexity of strings of the same length,
it does not work well for strings of different lengths because $\log P(\vec{c}_x)$ decreases with $\ell(x)$.
We would like to scale it to $Q(\vec{c}_x)$ such that
$Q$ approaches 0 given a random string.
We note that on the assumption of equal base frequency,
the average of $\log P(\vec{c}_x)$ can be approximated to
\begin{eqnarray*}
H(\lambda)&\triangleq&\sum_{\vec{c}}P(\vec{c})\cdot\sum_t\log p(c_t|\lambda)\\
&=&4^k\sum_{n=0}^{\infty}p(n|\lambda)\log p(n|\lambda)\\
&=&4^k\lambda(\log\lambda-1)-4^ke^{-\lambda}\sum_{n=0}^{\infty}\log n!\cdot\frac{\lambda^n}{n!}
\end{eqnarray*}
which is the negative entropy of $P$.
We can thus define
$$
Q(\vec{c}_x)\triangleq H(\lambda)-\log P(\vec{c}_x)=\sum_t\log c_x(t)!-f\left(\frac{\ell(x)}{4^k}\right)
$$
where
$$
f(\lambda)\triangleq4^ke^{-\lambda}\sum_{n=0}^\infty\log n!\cdot\frac{\lambda^n}{n!}
$$
$Q(\vec{c}_x)$ is higher for LC string $x$.

\subsection{Scoring low-complexity intervals}

%As base frequencies vary with the GC content
%and genome sequences are rarely random,
%$Q(\vec{c}_x)$ still increases with $\ell(x)$ on real data.
To put a threshold on the complexity, we finally use the following function to score string complexity:
\begin{equation}\label{eq:S}
S(\vec{c}_x)\triangleq Q(\vec{c}_x)-T\cdot\ell(x)=\sum_t\log c_x(t)!-T\cdot\ell(x)-f\left(\frac{\ell(x)}{4^k}\right)
\end{equation}
Threshold $T$ controls the level of complexity in the output.
It defaults to $0.6$, less than $\log 2$.
If $S(\vec{c}_x)>0$, $x$ is considered to contain an LC substring.
Note that we often do not want to classify the entire $x$ as an LC substring in this case
because the concatenation of a highly repetitive sequence and a random sequence
could still lead to a positive score.

Recall that we may use close intervals to represent substrings.
For convenience, we write $S(\vec{c}_{[i,j]})$ as $S(i,j)$.
In implementation, we precalculate $f(\ell/4^k)$ and introduce
\begin{eqnarray*}
U(i,j)&\triangleq&\sum_t\log c_{[i,j]}(t)!-T\cdot\ell([i,j])\\
&=&U(i,j-1)+\log c_{[i,j]}([j-k+1,j])-T
\end{eqnarray*}
We can thus compute the complexity scores of all prefixes of $[i,j]$
by scanning each base in the interval from left to right;
we can similarly compute all suffix scores from right to left.

\subsection{Finding low-complexity regions}

We say $x$ is a \emph{perfect LC string} (or \emph{perfect LC interval})
if $S(\vec{c}_x)>0$ and no substring of $x$ is scored higher than $S(\vec{c}_x)$;
say $x$ is a \emph{good LC string} (or \emph{good LC interval})
if $S(\vec{c}_x)>0$ and no prefix or suffix of $x$ is scored higher than $S(\vec{c}_x)$.
We can use $U(i,j)$ above to test if $[i,j]$ is a good LC interval in linear time.
If we apply this method to all intervals up to $w$ in length (5000bp by default),
we can find LC regions of context length up to $w$ in $O(w^2L)$ time.
The union of all good LC intervals marks the LC regions in a genome.

\begin{algorithm}[bt]
	\caption{Find LC interval ending at $j$}\label{algo:LC1}
	\begin{algorithmic}[1]
		\Procedure{FindStart}{$k,w,T,j,c'$}
			\State $B\gets${\sc Backward}$(k,w,T,j,c')$
			\State $j'_{\max}\gets-1$
			\For{$(i,v')\in B$ in the ascending order of $i$}
				\State {\bf continue if} $i<j'_{\max}$\Comment{this is an approximation}
				\State $j'\gets${\sc Forward}$(k,T,i,j,v')$
				\State \Return $i$ {\bf if} $j'=j$\Comment{$[i,j]$ is a good LC interval}
				\State $j'_{\max}\gets\max(j'_{\max},j')$
			\EndFor
			\State \Return $-1$\Comment{No good LC interval ending at $j$}
		\EndProcedure
		\Procedure{Backward}{$k,w,T,j,c'$}
			\State $u\gets 0$; $v_0\gets-1$; $u'\gets 0$
			\State $v_{\max}\gets0$; $i_{\max}\gets-1$; $c\gets[0]$
			\State $B\gets\emptyset$
			\For{$i\gets j$ {\bf to} $\max(j-w+1,k-1)$}\Comment{$i$ is descending}
				\State $t\gets[i-k+1,i]$\Comment{the $k$-mer ending at $i$}
				\State $c[t]\gets c[t]+1$
				\State $u\gets u+\log(c[t])-T$
				\State $v\gets u-f((j-i+1)/4^k)$\Comment{$v=S(i-k+1,j)$}
				\If{$v<v_0$ {\bf and} $v_0=v_{\max}$}
					\State $B\gets B\cup\{(i+1,v_{\max})\}$\Comment{a candidate start pos}
				\ElsIf{$v\ge v_{\max}$}
					\State $v_{\max}\gets v$; $i_{\max}\gets i$
				\ElsIf{$i_{\max}<0$}
					\State $u'\gets u'+\log(c'[t])-T$\Comment{$c'[t]\triangleq c_{[j-w+1,j]}(t)$}
					\State {\bf break if} $u'<0$\Comment{{\sc Forward}() wouldn't reach $j$}
				\EndIf
				\State $v_0\gets v$
			\EndFor
			\State $B\gets B\cup\{(i_{\max},v_{\max})\}$ {\bf if} $i_{\max}\ge0$
			\State\Return $B$
		\EndProcedure
		\Procedure{Forward}{$k,T,i_0,j,v'_{\max}$}
			\State $u\gets 0$; $v_{\max}\gets0$; $i_{\max}\gets-1$; $c\gets[0]$
			\For{$i\gets i_0$ {\bf to} $j$}
				\State $t\gets[i-k+1,i]$
				\State $c[t]\gets c[t]+1$
				\State $u\gets u+\log(c[t])-T$
				\State $v\gets u-f((i-i_0+1)/4^k)$
				\If{$v\ge v_{\max}$}
					\State $v_{\max}\gets v$; $i_{\max}\gets i$
				\EndIf
				\State {\bf break if} $v>v'_{\max}$
			\EndFor
			\State\Return $i_{\max}$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

Algorithm~\ref{algo:LC1} shows a faster way to find a good LC interval ending at $j$.
Function {\sc Backward}() scans backwardly from $j$ to $j-w+1$ to collect candidate start positions (line 22).
Variable $v$ is the complexity score of suffix $[i-k+1,j]$ (line 20).
By the definition of good LC interval, $i$ can only be a candidate start if $v$ is no less than all the suffixes visited before (line 21).
We also ignore a candidate start $i$ if $S(i,j)<S(i-1,j)$
because if $[i-1,j]$ is not a good LC interval, there must exist $i'>i$ such that $S(i',j)>S(i-1,j)<S(i,j)$, so $[i,j]$ would not be a good LC interval, either.
In addition, if suffix $[i,j]$ is enriched with $k$-mers unique in the full window $[j-w+1,j]$,
$[i,j]$ will not be a good LC interval (line 27) as there will exist $j'<j$ such that $S(i,j')>S(i,j)$.
The time complexity of {\sc Backward}() is $O(w)$.

Given a candidate start position $i$,
function {\sc Forward}() returns $j'=\argmax_{i<j'\le j} S(i,j')$.
$[i,j]$ will be a good LC interval if and only if $j'=j$ (Line 7).
We call {\sc Forward}() in the ascending order of candidate start positions (line 4).
We may skip a start position if it is contained in an interval found from previous {\sc Forward}() calls (line 5).
This is an approximation as it is possible for a good LC interval to start in another good interval.
An alternative heuristic is to only apply {\sc Forward}() to the smallest candidate start in $B$.
This leads to a guaranteed $O(w)$ with {\sc FindStart}().
In practice, the two algorithms have almost identical runtime.
We use Algorithm~\ref{algo:LC1} in longdust as it is closer to the exact algorithm.

Function {\sc FindStart}() finds the longest good LC interval ending at one position.
We apply the function to every position in the genome to find all good LC intervals.
We can skip $j$ if $[j-k+1,j]$ is unique in $[j-w+1,j]$ because the forward pass would not reach $j$ in this case.
We also introduce a heuristic to extend a good LC interval $[j-w,j-1]$ to $[j-w+1,j]$ without calling {\sc FindStart}().
We additionally use an X-drop heuristic~\citep{Altschul:1997vn} to avoid connecting two good LC intervals occasionally.

The overall longdust algorithm is inexact and may result in slightly different LC regions (21kb out of 278Mb in T2T-CHM13).
We run the algorithm on both the forward and the reverse strand of the input sequences and merge the resulting intervals.
The default longdust output is strand symmetric.

\subsection{Comparison to SDUST}

SDUST~\citep{Morgulis:2006aa} uses the following complexity scoring function:
$$
S_S(\vec{c}_x)=\frac{1}{\ell(x)}\sum_t\frac{c_x(t)(c_x(t)-1)}{2}-T
$$
This function grows linearly with $\ell(x)$ for $\ell(x)\ge4^k$,
while our scoring function grows more slowly in the logarithm scale.
The SDUST function is more likely to classify longer sequences as LC.

Furthermore, SDUST looks for perfect LC intervals rather than good LC intervals like longdust.
It cannot test whether an interval is perfect in linear time.
Instead, SDUST maintains the complete list of perfect intervals in window $[j-w+1,j]$
and tests a new candidate interval against the list.
The {\sc FindStart}() equivalent of SDUST is $O(w^3)$ in time, impractical for long windows.
SDUST hardcodes $k=3$ and uses $w=64$ by default for acceptable performance.

\subsection{Scoring with Shannon entropy}

Let $p_x(t)=c_x(t)/\ell(x)$.
The Shannon entropy of string $x$ is
$$
H(\vec{c}_x)\triangleq-\sum_tp_x(t)\log p_x(t)=\log\ell(x)-\frac{1}{\ell(x)}\sum_t c_x(t)\log c_x(t)
$$
When $\ell(x)\le4^k$, $H(\vec{c}_x)$ reaches the maximum value of $\log\ell(x)$ at $p_x(t)=1/\ell(x)$.
$H(\vec{c}_x)$ also grows with $\ell(x)$.
For $\ell(x)\le4^k$, define
$$
S_E(\vec{c}_x)\triangleq\log\ell(x)-H(\vec{c}_x)-T=\frac{1}{\ell(x)}\sum_t c_x(t)\log c_x(t)-T
$$
We adapted longdust for $S_E$ and found using $S_E$ is more than twice as slow.
We suspected some longdust heuristics did not work well with $S_E$, but we did not investigate further.

\section{Results}

\subsection{Low-complexity regions in T2T-CHM13}

\begin{table}[tb]
\caption{Command lines and resource usage for T2T-CHM13\label{tab:cmd}}
\begin{tabular*}{\columnwidth}{@{\extracolsep\fill}llll@{\extracolsep\fill}}
\toprule
Tool & CPU time & Mem (G) & Command line \\
\midrule
longdust & 1h3m   & 0.47 & (default) \\
SDUST    & 4m15s  & 0.23 & {\tt -t30} \\
pytrf    & 2h39m  & 0.70 & {\tt -M500} \\
TRF      & 12h52m & 7.49 & {\tt 2 7 7 80 10 50 500 -l12} \\
TANTAN   & 32m50s & 1.28 & {\tt -w500 -s.85} \\
ULTRA    & 146h4m & 33.31& {\tt -p500 -t16} \\
\botrule
\end{tabular*}
\begin{tablenotes}\setlength\itemsep{0.0em}
Performance measured on a Linux server equipped with Intel Xeon Gold 6130 CPU and 512GB memory.
\end{tablenotes}
\end{table}

\begin{figure}[tb]
\includegraphics[width=\columnwidth]{len}
\caption{Lengths of low-complexity regions.
Low-complexity (LC) regions identified by each tool are first intersected with centromeric satellite annotation.
The remainder is then intersected with longdust, TRF and SDUST in order.
There are no overlaps between stacks.
The total height is the length of LC regions found by each tool.
Alternative settings --
``TRF-c4'': requiring $\ge4$ copies of the repeat unit;
``ULTRA-s30'': requiring score $\ge30$ in ULTRA output;
``TANTAN-95'': run with ``{\tt -s.95}'' for more stringent output.
}\label{fig:len}
\end{figure}

We applied longdust, SDUST v0.1~\citep{Morgulis:2006aa},
pytrf v1.4.2~\citep{Du:2025aa},
TRF v4.10~\citep{Benson:1999aa},
TANTAN v51~\citep{Frith:2011aa},
and ULTRA v1.20~\citep{Olson:2024aa}
to the T2T-CHM13 human genome~\citep{Nurk:2022up}.
Command lines for TRF, TANTAN and ULTRA were adopted from \citet{Olson:2024aa},
with the maximum period set to 500 (Table~\ref{tab:cmd}).
Notably, TRF would not finish in days with the default option {\tt -l}.
An even larger {\tt -l} helps performance at the cost of memory.

Longdust finds 277.1Mb of LC regions with 224.3Mb overlapping with centromeric satellite annotated by the telomere-to-telomere (T2T) consortium (Fig.~\ref{fig:len}).
Of the remaining 52.7Mb, 34.1Mb overlaps with TRF; 15.4Mb of the remainder (18.6Mb) is found by SDUST.
Only 3.2Mb is left, suggesting most longdust LC regions fall in centromeres or are found by TRF or SDUST.

TRF, the most popular tandem repeat finder, finds 274.5Mb of tandem repeats,
244.0Mb of which have $\ge4$ copies of repeat units.
97.9\% of the 244.0Mb are identified by longdust.
TRF additionally reports tandem repeats with $<4$ repeat units.
Only 14.8\% of them overlap with longdust results.
Longdust misses repeats with low copy numbers.
In fact, under the default threshold $T=0.6$,
the minimum numbers of exact copies longdust can find is approximately:
$$
3+\frac{k-1}{r}+\frac{3T-\log2-\log3}{\log4-T}\approx3.01+\frac{k-1}{r}
$$
This assumes the repeat unit length $r>k$, $f(\cdot)=0$ and all $k$-mers are unique within the repeat unit.

As to other tools, ULTRA outputs the largest tandem repeat regions (Fig.~\ref{fig:len}).
Nevertheless, if we raise its score threshold to 30,
the total region length is similar to that of TRF.
TANTAN also reports many regions not found by longdust, TRF or SDUST.
Increasing its probability threshold only marginally change the relative portions.
Pytrf cannot effectively identify alpha satellite with $\sim$170bp repeat units,
even though it was set to find tandem repeats with unit up to 500bp.
SDUST is the only other method that looks for LC regions not limited to tandem repeats.
With a 64bp window size, it naturally misses tandem repeats with long units, including all alpha satellite.

\subsection{Low-complexity regions in a gorilla genome}

The near T2T gorilla genome (AC:GCF\_029281585.2; \citealp{Yoo:2025aa}) is 3546Mb in size, 428Mb larger than the human T2T-CHM13 genome.
We ran longdust on the gorilla genome for 1.4 hours and found 656.8Mb of LC regions.
That is 379.7Mb larger than the LC regions in T2T-CHM13.
The genome size difference is primarily driven by LC regions.

To further confirm this observation,
we extracted 298.8Mb of regions in the gorilla genome that are $\ge10$kb in length without any 51-mer exact matches to 472 human genomes~\citep{Li:2024ac}.
99.7\% of them are marked as LC regions by longdust and none of them are alpha satellite.
95.8\% of these gorilla-specific regions are distributed within 15Mb from telomeres,
broadly in line with \citet{Yoo:2025aa}.
We also ran TRF on the gorilla genome.
It did not finish in 30 hours even with option ``{\tt -l20}'' which is supposed to reduce runtime.

\section{Discussions}

Implemented in the C programming language, longdust is a fast and lightweight command-line tool for
identifying low-complexity regions.
It rarely finds LC regions not reported by TRF plus SDUST
and can recover more tandem repeats with $\ge4$ copies of repeat units.
Longdust provides basic APIs in C and can also be used as a programming library.

From the theoretical point of view, longdust uses an approximate algorithm.
It tests LC intervals ending at each position with Algorithm~\ref{algo:LC1},
but has not sufficiently exploited dependencies between positions.
It will be interesting to see whether there is an exact $O(wL)$ algorithm under the longdust formulation
or a meaningful alternative formulation that leads fast implementations.

A major practical limitation of longdust is the restricted window size.
The genome of Woodhouse's scrub jays, for example, contains satellite with a 12kb repeat unit~\citep{Edwards:2025aa}.
This would be missed by longdust under the default setting.
Increasing the window size would make longdust considerably slower.
This is partly due to the $O(wL)$ time complexity and partly due to the speedup strategies in longdust that are more effective given $\ell(x)\ll 4^k$.
It would be ideal to have an algorithm that remains efficient given large windows or, better, does not require specified window sizes.

\section*{Acknowledgments}

We would like to thank Qian Qin for evaluating the effect of low-complexity regions in structural variant calling.

\section*{Author contributions}

H.L. conceived the project, implemented the method, analyzed the data and drafted the manuscript.
B.L. prototyped the algorithm.

\section*{Conflict of interest}

None declared.

\section*{Funding}

This work is supported by US National Institute of Health grant R01HG010040, R01HG014175, U01HG013748, U41HG010972, and U24CA294203 (to H.L.).

\section*{Data availability}

\url{https://github.com/lh3/longdust}

\bibliographystyle{apalike}
{\sffamily\small
\bibliography{longdust}}

\end{document}
